<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Deep Residual Learning for Image Recognition · Bocheng Zeng's BLOG</title><meta name="description" content="Deep Residual Learning for Image Recognition - Bocheng Zeng"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/hermes.css"><link rel="search" type="application/opensearchdescription+xml" href="https://zengbocheng.github.io/atom.xml" title="Bocheng Zeng's BLOG"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Bocheng Zeng's BLOG" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="/atom.xml" target="_self">FEED</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Deep Residual Learning for Image Recognition</h1><div class="post-info">May 15, 2022</div><div class="post-content"><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a><strong>1 Introduction</strong></h1><p>目前很多实验表明，随着网络深度的增加，精度逐渐达到饱和，然后迅速下降。这显然不是由于过拟合造成的。训练准确性的下降表明，不是所有的网络都容易训练。为了解决这个问题，作者提出了一种<strong>深度残差学习框架</strong>。<span id="more"></span>假设需要拟合的映射为$\mathcal{H}(\bold{x})$ ，我们使用堆叠的非线性层拟合$\mathcal{F}(\bold{x}) := \mathcal{H}(\bold x) - \bold x$。原始映射可以通过$\mathcal{F}(\bold x) + \bold x$还原。在ImageNet、CIFAR-10和COCO等数据集上的实验，证明了（1）深度残差网络比普通深度网络容易训练；（2）深度残差网络的精度随着深度的提高而提高。</p>
<p><img src="https://s1.ax1x.com/2022/05/15/O2seZn.jpg" alt="https://s1.ax1x.com/2022/05/15/O2seZn.jpg"></p>
<h1 id="2-Relation-Work"><a href="#2-Relation-Work" class="headerlink" title="2 Relation Work"></a><strong>2 Relation Work</strong></h1><p>residual representations and shortcut connections.</p>
<h1 id="3-Deep-Residual-Learning"><a href="#3-Deep-Residual-Learning" class="headerlink" title="3 Deep Residual Learning"></a><strong>3 Deep Residual Learning</strong></h1><p>考虑一个浅层网络A和一个深层网络B，其中深层网络B只比浅层网络A多了一些额外非线性层C。理论上，存在这样的深层网络B，它的前部分A’与A完全一致，后部分C为恒等映射。这说明一个更深的模型，理论上对其进行训练不会得到比浅层模型更差的结果。但实验表明，难以训练得到这样一个深层模型。这说明，可能是优化器难以使用多个非线性层来拟合恒等映射。而对于残差网络来说，优化器只需要将多个非线性层的权重置零就可以得到恒等映射。</p>
<p><img src="https://s1.ax1x.com/2022/05/15/O2hnoQ.jpg" alt="https://s1.ax1x.com/2022/05/15/O2hnoQ.jpg"></p>
<p>如果 $\mathcal{F}(\bold x)$与 $\bold x$的维度相同，残差块可以写成 $\bold y = \mathcal{F}(\bold x, \{W_i\}) + \bold x$ 如果维度不同，可以使用W_s来匹配维度 $\bold y = \mathcal{F}(\bold x, \{W_i\}) + W_s\bold x$其中 $\bold x, \bold y$分别为输入输出， $\mathcal{F}$是残差映射（即可以是全连接层，也可以是卷积层）。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a><strong>4 Experiments</strong></h1><h2 id="4-1-ImageNet"><a href="#4-1-ImageNet" class="headerlink" title="4.1 ImageNet"></a><strong>4.1 ImageNet</strong></h2><p>作者分别构建了18层、34层的普通网络和残差网络，二者的结构、参数数量保持一致。</p>
<p><img src="https://s1.ax1x.com/2022/05/17/O4ygoQ.jpg" alt="https://s1.ax1x.com/2022/05/17/O4ygoQ.jpg"></p>
<p>34层的普通网络出现了训练精度下降的问题，而34层的残差网络比18层的残差网络的训练误差要更低，并且能够泛化到验证数据。</p>
<p><img src="https://s1.ax1x.com/2022/05/17/O44bT0.jpg" alt="https://s1.ax1x.com/2022/05/17/O44bT0.jpg"></p>
<p>ResNet随着深度的增加，准确度越高。</p>
<h2 id="4-2-CIFAR-10"><a href="#4-2-CIFAR-10" class="headerlink" title="4.2 CIFAR-10"></a><strong>4.2 CIFAR-10</strong></h2><p>研究极深网络的表现。</p>
<p><img src="https://s1.ax1x.com/2022/05/17/O5Fy8O.jpg" alt="https://s1.ax1x.com/2022/05/17/O5Fy8O.jpg"></p>
<p>在普通网络上，依然出现了训练准确度下降的问题，而ResNet表现良好。</p>
</div></article></div></main><footer><div class="paginator"><a class="prev" href="/2022/10/17/%E6%95%B0%E5%AD%A6%E4%B8%AD%E7%9A%84%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97/">PREV</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'bochengzeng';
var disqus_identifier = '2022/05/15/Deep Residual Learning for Image Recognition/';
var disqus_title = 'Deep Residual Learning for Image Recognition';
var disqus_url = 'https://zengbocheng.github.io/2022/05/15/Deep Residual Learning for Image Recognition/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2022 <a href="https://zengbocheng.github.io">Bocheng Zeng</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/claymcleod/hexo-theme-hermes" target="_blank">hexo-theme-hermes</a>. </p><p>Logo made by <a target="_blank" rel="noopener" href="https://www.flaticon.com/authors/freepik">Freepik</a> from <a target="_blank" rel="noopener" href="https://flaticon.com">www.flaticon.com</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>